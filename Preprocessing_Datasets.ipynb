{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IPUMS dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading dataset（https://www.ipums.org/）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usadf = pd.read_csv('usa_00011.csv')\n",
    "usadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = ['YEAR','MET2013','PROPTX99','COSTELEC','HHINCOME','AGE','BPLD','ANCESTR1','OCC2010',\n",
    "        'INCEARN','PRESGL','PRENT','PWMET13','UHRSWORK']\n",
    "cols_to_drop = [col for col in usadf.columns if col not in col1]\n",
    "usadf = usadf.drop(cols_to_drop, axis=1)\n",
    "usadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usadf = usadf.loc[(usadf['MET2013'] != 0)]\n",
    "usadf = usadf.loc[(usadf['PROPTX99'] != 0)]\n",
    "usadf = usadf.loc[(usadf['BPLD'] != 99900)]\n",
    "usadf = usadf.loc[(usadf['ANCESTR1'] != 999)]\n",
    "usadf = usadf.loc[(usadf['ANCESTR1'] != 998)]\n",
    "usadf = usadf.loc[(usadf['ANCESTR1'] != 996)]\n",
    "usadf = usadf.loc[(usadf['PRENT'] != 0)]\n",
    "usadf = usadf.loc[(usadf['PWMET13'] != 0)]\n",
    "usadf = usadf.loc[(usadf['AGE'] >= 16)]\n",
    "usadf = usadf.loc[(usadf['AGE'] <= 79)]\n",
    "usadf = usadf.loc[(usadf['YEAR'] > 2019)]\n",
    "usadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(usadf.loc[(usadf['YEAR'] == 2019)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "col1 = usadf.columns\n",
    "for i in range(len(col1)):\n",
    "    val_att = set(usadf[col1[i]])\n",
    "    print(col1[i],len(set(usadf[col1[i]])))\n",
    "    if len(set(usadf[col1[i]]))<64:\n",
    "        usadf = usadf.drop(col1[i], axis=1)\n",
    "    # print(col1[i], set(usadf[col1[i]]), len(set(usadf[col1[i]])))\n",
    "    # lbe = LabelEncoder()\n",
    "    # usadf[[col1[i]]]=usadf[[col1[i]]].apply(lbe.fit_transform)\n",
    "    # print(col1[i], set(usadf[col1[i]]), len(set(usadf[col1[i]])))\n",
    "    # print(lbe.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "col1 = ['MET2013','PROPTX99','COSTELEC','HHINCOME','AGE','BPLD','ANCESTR1','OCC2010',\n",
    "        'INCEARN','PRESGL','PRENT','PWMET13','UHRSWORK']\n",
    "for i in range(len(col1)):\n",
    "    val_att = set(usadf[col1[i]])\n",
    "    print(col1[i], set(usadf[col1[i]]), len(set(usadf[col1[i]])))\n",
    "    lbe = LabelEncoder()\n",
    "    usadf[[col1[i]]]=usadf[[col1[i]]].apply(lbe.fit_transform)\n",
    "    print(col1[i], set(usadf[col1[i]]), len(set(usadf[col1[i]])))\n",
    "    print(lbe.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = ['MET2013','PROPTX99','COSTELEC','HHINCOME','AGE','BPLD','ANCESTR1','OCC2010',\n",
    "        'INCEARN','PRESGL','PRENT','PWMET13','UHRSWORK']\n",
    "for i in range(len(col1)):\n",
    "    block = np.round((max(usadf[col1[i]])-min(usadf[col1[i]]))/64)\n",
    "    usadf[col1[i]] = (usadf[col1[i]]+abs(min(usadf[col1[i]])))//block\n",
    "    usadf[col1[i]] = np.clip(usadf[col1[i]], 0, 63)\n",
    "    usadf[col1[i]] = usadf[col1[i]].astype(int)\n",
    "    print(max(usadf[col1[i]]),min(usadf[col1[i]]))\n",
    "    # print(col1[i], set(usadf[col1[i]]), len(set(usadf[col1[i]])))\n",
    "    # lbe = LabelEncoder()\n",
    "    # usadf[[col1[i]]]=usadf[[col1[i]]].apply(lbe.fit_transform)\n",
    "    # print(col1[i], set(usadf[col1[i]]), len(set(usadf[col1[i]])))\n",
    "    # print(lbe.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "x = np.arange(64)\n",
    "plt.hist(usadf['HHWT'], bins = len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(usadf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = ['MET2013','COSTELEC','AGE','BPLD','ANCESTR1','OCC2010','INCEARN','PRENT','PWMET13','UHRSWORK']\n",
    "cols_to_drop = [col for col in usadf.columns if col not in col1]\n",
    "usadf = usadf.drop(cols_to_drop, axis=1)\n",
    "usadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usadf.columns = ['attribute' + str(i) for i in range(len(usadf.columns))]\n",
    "usadf = usadf.reset_index(drop=True)\n",
    "usadf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子，以确保结果可重复\n",
    "np.random.seed(42)\n",
    "\n",
    "# 生成100000行2列的数据\n",
    "rows = len(usadf)\n",
    "cols = 4\n",
    "\n",
    "# 生成属性名\n",
    "attributes = ['uniform1', 'normal1', 'laplace1', 'prob1']\n",
    "\n",
    "# 生成数据\n",
    "uniform_data = np.random.randint(1, 11, size=(rows, 1))\n",
    "normal_data = np.random.normal(5.5, 2.5, size=(rows, 1))\n",
    "normal_data = np.clip(normal_data, 1, 10)\n",
    "laplace_data = np.random.laplace(5, 2, size=(rows, 1))\n",
    "laplace_data = np.clip(laplace_data, 1, 10)\n",
    "prob_data = np.random.choice([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], size=(rows, 1), p=[0.18, 0.18, 0.18, 0.0925, 0.0925, 0.0925, 0.0925, 0.03, 0.03, 0.03])\n",
    "\n",
    "# 将浮点型数据转换为整型\n",
    "uniform_data = uniform_data.astype(int)\n",
    "normal_data = normal_data.astype(int)\n",
    "laplace_data = laplace_data.astype(int)\n",
    "prob_data = prob_data.astype(int)\n",
    "\n",
    "# 创建DataFrame\n",
    "df2 = pd.DataFrame(np.concatenate([uniform_data, normal_data, laplace_data, prob_data], axis=1), columns=attributes)\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([usadf, df2], axis=1)\n",
    "merged_df.to_csv('1000000_ipums_64.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "acs_padf = pd.read_csv('ss15pusa.csv')\n",
    "acs_padf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acs_pbdf = pd.read_csv('ss15pusb.csv')\n",
    "acs_pbdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acsdf = pd.concat([acs_padf, acs_pbdf], axis=0)\n",
    "\n",
    "# 如果需要重新设置合并后的索引\n",
    "acsdf = acsdf.reset_index(drop=True)\n",
    "acsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = ['ST','AGEP','JWMNP','WKHP','ANC1P','FOD1P','INDP','OCCP','POBP','pwgtp80']\n",
    "cols_to_drop = [col for col in acsdf.columns if col not in col1]\n",
    "acsdf = acsdf.drop(cols_to_drop, axis=1)\n",
    "acsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acsdf.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "x = np.arange(64)\n",
    "plt.hist(acsdf['pwgtp80'], bins = len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acsdf = acsdf.loc[(acsdf['ANC1P'] != 999)]\n",
    "acsdf = acsdf.loc[(acsdf['ANC1P'] != 998)]\n",
    "acsdf = acsdf.loc[(acsdf['ANC1P'] != 997)]\n",
    "acsdf = acsdf.loc[(acsdf['AGEP'] >= 10)]\n",
    "acsdf = acsdf.loc[(acsdf['AGEP'] <= 73)]\n",
    "acsdf = acsdf.reset_index(drop=True)\n",
    "acsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acsdf = acsdf.interpolate(method = 'linear', axis = 0) # 将通过 linear 插值使用同一列的中间值作为填充\n",
    "acsdf = acsdf.fillna(method = 'backfill', axis = 0) # 将通过前向填充 (ffill) 方法用同一列的后一个数作为填充\n",
    "acsdf = acsdf.fillna(method = 'ffill', axis = 0) # 将通过 linear 插值使用同一列的中间值作为填充\n",
    "acsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "col1 = acsdf.columns\n",
    "for i in range(len(col1)):\n",
    "    val_att = set(acsdf[col1[i]])\n",
    "    print(col1[i], set(acsdf[col1[i]]), len(set(acsdf[col1[i]])))\n",
    "    lbe = LabelEncoder()\n",
    "    acsdf[[col1[i]]]=acsdf[[col1[i]]].apply(lbe.fit_transform)\n",
    "    print(col1[i], set(acsdf[col1[i]]), len(set(acsdf[col1[i]])))\n",
    "    print(lbe.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = acsdf.columns\n",
    "for i in range(len(col1)):\n",
    "    block = np.round((max(acsdf[col1[i]])-min(acsdf[col1[i]]))/64)\n",
    "    acsdf[col1[i]] = (acsdf[col1[i]]+abs(min(acsdf[col1[i]])))//block\n",
    "    acsdf[col1[i]] = np.clip(acsdf[col1[i]], 0, 63)\n",
    "    acsdf[col1[i]] = acsdf[col1[i]].astype(int)\n",
    "    print(max(acsdf[col1[i]]),min(acsdf[col1[i]]))\n",
    "    # print(col1[i], set(acsdf[col1[i]]), len(set(acsdf[col1[i]])))\n",
    "    # lbe = LabelEncoder()\n",
    "    # acsdf[[col1[i]]]=acsdf[[col1[i]]].apply(lbe.fit_transform)\n",
    "    # print(col1[i], set(acsdf[col1[i]]), len(set(acsdf[col1[i]])))\n",
    "    # print(lbe.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acsdf.columns = ['attribute' + str(i) for i in range(len(acsdf.columns))]\n",
    "acsdf = acsdf.reset_index(drop=True)\n",
    "acsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子，以确保结果可重复\n",
    "np.random.seed(42)\n",
    "\n",
    "# 生成100000行2列的数据\n",
    "rows = len(acsdf)\n",
    "cols = 4\n",
    "\n",
    "# 生成属性名\n",
    "attributes = ['uniform1', 'normal1', 'laplace1', 'prob1']\n",
    "\n",
    "# 生成数据\n",
    "uniform_data = np.random.randint(1, 11, size=(rows, 1))\n",
    "normal_data = np.random.normal(5.5, 2.5, size=(rows, 1))\n",
    "normal_data = np.clip(normal_data, 1, 10)\n",
    "laplace_data = np.random.laplace(5, 2, size=(rows, 1))\n",
    "laplace_data = np.clip(laplace_data, 1, 10)\n",
    "prob_data = np.random.choice([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], size=(rows, 1), p=[0.18, 0.18, 0.18, 0.0925, 0.0925, 0.0925, 0.0925, 0.03, 0.03, 0.03])\n",
    "\n",
    "# 将浮点型数据转换为整型\n",
    "uniform_data = uniform_data.astype(int)\n",
    "normal_data = normal_data.astype(int)\n",
    "laplace_data = laplace_data.astype(int)\n",
    "prob_data = prob_data.astype(int)\n",
    "\n",
    "# 创建DataFrame\n",
    "df2 = pd.DataFrame(np.concatenate([uniform_data, normal_data, laplace_data, prob_data], axis=1), columns=attributes)\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([acsdf, df2], axis=1)\n",
    "merged_df.to_csv('1000000_acs_64.csv',index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loan dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "loan_adf = pd.read_csv('accepted_2007_to_2018Q4.csv')\n",
    "loan_adf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_adf = loan_adf.fillna(999999999)\n",
    "loan_adf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "col1 = loan_adf.columns\n",
    "for i in range(len(col1)):\n",
    "    val_att = set(loan_adf[col1[i]])\n",
    "    print(col1[i],len(set(loan_adf[col1[i]])))\n",
    "    # if len(set(loan_adf[col1[i]]))<64:\n",
    "    #     loan_adf = loan_adf.drop(col1[i], axis=1)\n",
    "    # print(col1[i], set(usadf[col1[i]]), len(set(usadf[col1[i]])))\n",
    "    # lbe = LabelEncoder()\n",
    "    # usadf[[col1[i]]]=usadf[[col1[i]]].apply(lbe.fit_transform)\n",
    "    # print(col1[i], set(usadf[col1[i]]), len(set(usadf[col1[i]])))\n",
    "    # print(lbe.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_adf['issue_d'] = loan_adf['issue_d'].replace(999999999, 'z')\n",
    "loan_adf['zip_code'] = loan_adf['zip_code'].replace(999999999, 'z')\n",
    "loan_adf['addr_state'] = loan_adf['addr_state'].replace(999999999, 'z')\n",
    "loan_adf['earliest_cr_line'] = loan_adf['earliest_cr_line'].replace(999999999, 'z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "col1 = ['loan_amnt', 'installment', 'issue_d', 'zip_code', 'addr_state', 'dti', 'earliest_cr_line', 'open_acc', 'avg_cur_bal', 'total_pymnt']\n",
    "for i in range(len(col1)):\n",
    "    print(col1[i], set(loan_adf[col1[i]]), len(set(loan_adf[col1[i]])))\n",
    "    lbe = LabelEncoder()\n",
    "    loan_adf[[col1[i]]]=loan_adf[[col1[i]]].apply(lbe.fit_transform)\n",
    "    print(col1[i], set(loan_adf[col1[i]]), len(set(loan_adf[col1[i]])))\n",
    "    print(lbe.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = ['loan_amnt', 'installment', 'issue_d', 'zip_code', 'addr_state', 'dti', 'earliest_cr_line', 'open_acc', 'avg_cur_bal', 'total_pymnt']\n",
    "cols_to_drop = [col for col in loan_adf.columns if col not in col1]\n",
    "loan_adf = loan_adf.drop(cols_to_drop, axis=1)\n",
    "loan_adf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_adf = loan_adf.loc[(loan_adf['loan_amnt'] != max(loan_adf['loan_amnt']))]\n",
    "loan_adf = loan_adf.loc[(loan_adf['installment'] != max(loan_adf['installment']))]\n",
    "loan_adf = loan_adf.loc[(loan_adf['issue_d'] != max(loan_adf['issue_d']))]\n",
    "loan_adf = loan_adf.loc[(loan_adf['zip_code'] != max(loan_adf['zip_code']))]\n",
    "loan_adf = loan_adf.loc[(loan_adf['addr_state'] != max(loan_adf['addr_state']))]\n",
    "loan_adf = loan_adf.loc[(loan_adf['dti'] != max(loan_adf['dti']))]\n",
    "loan_adf = loan_adf.loc[(loan_adf['earliest_cr_line'] != max(loan_adf['earliest_cr_line']))]\n",
    "loan_adf = loan_adf.loc[(loan_adf['open_acc'] != max(loan_adf['open_acc']))]\n",
    "loan_adf = loan_adf.loc[(loan_adf['avg_cur_bal'] != max(loan_adf['avg_cur_bal']))]\n",
    "loan_adf = loan_adf.loc[(loan_adf['total_pymnt'] != max(loan_adf['total_pymnt']))]\n",
    "loan_adf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col1 = loan_adf.columns\n",
    "for i in range(len(col1)):\n",
    "    block = np.round((max(loan_adf[col1[i]])-min(loan_adf[col1[i]]))/64)\n",
    "    loan_adf[col1[i]] = (loan_adf[col1[i]]-min(loan_adf[col1[i]]))//block\n",
    "    loan_adf[col1[i]] = np.clip(loan_adf[col1[i]], 0, 63)\n",
    "    loan_adf[col1[i]] = loan_adf[col1[i]].astype(int)\n",
    "    print(max(loan_adf[col1[i]]),min(loan_adf[col1[i]]))\n",
    "    # print(col1[i], set(loan_adf[col1[i]]), len(set(loan_adf[col1[i]])))\n",
    "    # lbe = LabelEncoder()\n",
    "    # loan_adf[[col1[i]]]=loan_adf[[col1[i]]].apply(lbe.fit_transform)\n",
    "    # print(col1[i], set(loan_adf[col1[i]]), len(set(loan_adf[col1[i]])))\n",
    "    # print(lbe.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "x = np.arange(64)\n",
    "plt.hist(loan_adf['loan_amnt'], bins = len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_adf.columns = ['attribute' + str(i) for i in range(len(loan_adf.columns))]\n",
    "loan_adf = loan_adf.reset_index(drop=True)\n",
    "loan_adf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子，以确保结果可重复\n",
    "np.random.seed(42)\n",
    "\n",
    "# 生成100000行2列的数据\n",
    "rows = len(loan_adf)\n",
    "cols = 4\n",
    "\n",
    "# 生成属性名\n",
    "attributes = ['uniform1', 'normal1', 'laplace1', 'prob1']\n",
    "\n",
    "# 生成数据\n",
    "uniform_data = np.random.randint(1, 11, size=(rows, 1))\n",
    "normal_data = np.random.normal(5.5, 2.5, size=(rows, 1))\n",
    "normal_data = np.clip(normal_data, 1, 10)\n",
    "laplace_data = np.random.laplace(5, 2, size=(rows, 1))\n",
    "laplace_data = np.clip(laplace_data, 1, 10)\n",
    "prob_data = np.random.choice([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], size=(rows, 1), p=[0.18, 0.18, 0.18, 0.0925, 0.0925, 0.0925, 0.0925, 0.03, 0.03, 0.03])\n",
    "\n",
    "# 将浮点型数据转换为整型\n",
    "uniform_data = uniform_data.astype(int)\n",
    "normal_data = normal_data.astype(int)\n",
    "laplace_data = laplace_data.astype(int)\n",
    "prob_data = prob_data.astype(int)\n",
    "\n",
    "# 创建DataFrame\n",
    "df2 = pd.DataFrame(np.concatenate([uniform_data, normal_data, laplace_data, prob_data], axis=1), columns=attributes)\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([loan_adf, df2], axis=1)\n",
    "merged_df.to_csv('1000000_loan_64.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as ss\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0, 64)\n",
    "xU, xL = x + 0.5, x - 0.5 \n",
    "prob = ss.norm.cdf(xU, scale = 1) - ss.norm.cdf(xL, scale = 1)\n",
    "prob = prob / prob.sum() # normalize the probabilities so their sum is 1\n",
    "nums = np.random.choice(x, size = 10000, p = prob)\n",
    "plt.hist(nums, bins = len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nums = nums+5\n",
    "print(set(nums))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "x = np.arange(1, 11)\n",
    "x2 = numpy.random.randint(low=1, high=11, size=45223, dtype='l')\n",
    "print(set(x2), len(set(x2)))\n",
    "plt.hist(x2, bins = len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_epsilon = pd.DataFrame()\n",
    "df_epsilon['uniform1'] = x2\n",
    "df_epsilon['normal1'] = nums\n",
    "df_epsilon.to_csv('db_adults_epsilon.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# 设置随机种子，以确保结果可重复\n",
    "np.random.seed(42)\n",
    "# 生成100000行10列的数据\n",
    "rows = 1000000\n",
    "cols = 10\n",
    "\n",
    "# 生成属性名\n",
    "attributes = ['attribute' + str(i) for i in range(cols)]\n",
    "\n",
    "# 生成数据\n",
    "# data = np.random.uniform(0, 64, size=(rows, cols))\n",
    "# data = np.random.normal(loc=31.5, scale=15, size=(rows, cols))\n",
    "# data = np.clip(data, 0, 63)\n",
    "data = np.random.laplace(loc=0, scale=1, size=(rows, cols))\n",
    "data_min = np.min(data)\n",
    "data_max = np.max(data)\n",
    "data = (data - data_min) / (data_max - data_min) * 63\n",
    "data = np.clip(data, 0, 63)\n",
    "data = data.astype(int)\n",
    "\n",
    "# 创建DataFrame\n",
    "df1 = pd.DataFrame(data, columns=attributes)\n",
    "\n",
    "# 打印DataFrame\n",
    "print(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.to_csv('uniform.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子，以确保结果可重复\n",
    "np.random.seed(42)\n",
    "\n",
    "# 生成100000行2列的数据\n",
    "rows = 1000000\n",
    "cols = 4\n",
    "\n",
    "# 生成属性名\n",
    "attributes = ['uniform1', 'normal1', 'laplace1', 'prob1']\n",
    "\n",
    "# 生成数据\n",
    "uniform_data = np.random.randint(1, 11, size=(rows, 1))\n",
    "normal_data = np.random.normal(5.5, 2.5, size=(rows, 1))\n",
    "normal_data = np.clip(normal_data, 1, 10)\n",
    "laplace_data = np.random.laplace(5, 2, size=(rows, 1))\n",
    "laplace_data = np.clip(laplace_data, 1, 10)\n",
    "prob_data = np.random.choice([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], size=(rows, 1), p=[0.18, 0.18, 0.18, 0.0925, 0.0925, 0.0925, 0.0925, 0.03, 0.03, 0.03])\n",
    "\n",
    "# 将浮点型数据转换为整型\n",
    "uniform_data = uniform_data.astype(int)\n",
    "normal_data = normal_data.astype(int)\n",
    "laplace_data = laplace_data.astype(int)\n",
    "prob_data = prob_data.astype(int)\n",
    "\n",
    "# 创建DataFrame\n",
    "df2 = pd.DataFrame(np.concatenate([uniform_data, normal_data, laplace_data, prob_data], axis=1), columns=attributes)\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of samples and the number of attributes\n",
    "num_samples = 1000000\n",
    "num_attributes = 10\n",
    "\n",
    "# Generate random covariance matrix\n",
    "covariance_matrix = np.random.rand(num_attributes, num_attributes)\n",
    "\n",
    "# Set half of the covariances to be less than 0.3\n",
    "covariance_matrix[covariance_matrix < 0.6] = np.random.uniform(0, 0.2, size=np.sum(covariance_matrix < 0.6))\n",
    "\n",
    "# Set remaining covariances to be greater than 0.3\n",
    "covariance_matrix[covariance_matrix >= 0.6] = np.random.uniform(0.2, 0.8, size=np.sum(covariance_matrix >= 0.6))\n",
    "\n",
    "# Make the covariance matrix symmetric\n",
    "covariance_matrix = np.triu(covariance_matrix) + np.triu(covariance_matrix, 1).T\n",
    "\n",
    "# 计算特征值分解\n",
    "eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "# 将负特征值设置为零\n",
    "eigenvalues[eigenvalues < 0] = 0\n",
    "\n",
    "# 重新构造对称半正定矩阵\n",
    "covariance_matrix = np.dot(eigenvectors, np.dot(np.diag(eigenvalues), eigenvectors.T))\n",
    "np.fill_diagonal(covariance_matrix, 1)\n",
    "\n",
    "# Generate the dataset\n",
    "data = np.random.multivariate_normal(mean=[0] * num_attributes, cov=covariance_matrix, size=num_samples)\n",
    "\n",
    "# Scale the data to the range of 0-63\n",
    "data_min = np.min(data)\n",
    "data_max = np.max(data)\n",
    "scaled_data = (data - data_min) / (data_max - data_min) * 63\n",
    "\n",
    "# Round the scaled data to the nearest integer\n",
    "rounded_data = np.round(scaled_data)\n",
    "rounded_data = rounded_data.astype(int)\n",
    "print(np.max(rounded_data))\n",
    "print(np.min(rounded_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "num_samples = 1000000\n",
    "num_attributes = 10\n",
    "\n",
    "# Generate random covariance matrix\n",
    "cov = np.random.rand(num_attributes, num_attributes)\n",
    "\n",
    "# Set half of the covariances to be less than 0.3\n",
    "cov[cov < 0.6] = np.random.uniform(0, 0.2, size=np.sum(cov < 0.6))\n",
    "\n",
    "# Set remaining covariances to be greater than 0.3\n",
    "cov[cov >= 0.6] = np.random.uniform(0.2, 0.8, size=np.sum(cov >= 0.6))\n",
    "\n",
    "# Make the covariance matrix symmetric\n",
    "cov = np.triu(cov) + np.triu(cov, 1).T\n",
    "# 计算特征值分解\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "\n",
    "# 将负特征值设置为零\n",
    "eigenvalues[eigenvalues < 0] = 0\n",
    "\n",
    "# 重新构造对称半正定矩阵\n",
    "cov = np.dot(eigenvectors, np.dot(np.diag(eigenvalues), eigenvectors.T))\n",
    "np.fill_diagonal(cov, 1)\n",
    "# Generate samples from the multivariate uniform distribution\n",
    "data = np.random.uniform(low=0, high=1, size=(num_samples, num_attributes))\n",
    "\n",
    "# Apply the covariance matrix to the data\n",
    "data = np.dot(data, np.linalg.cholesky(cov).T)\n",
    "\n",
    "# Scale the data to the range of 0-63\n",
    "data_min = np.min(data)\n",
    "data_max = np.max(data)\n",
    "scaled_data = (data - data_min) / (data_max - data_min) * 63\n",
    "\n",
    "# Round the scaled data to the nearest integer\n",
    "rounded_data = np.round(scaled_data)\n",
    "rounded_data = rounded_data.astype(int)\n",
    "print(np.max(rounded_data))\n",
    "print(np.min(rounded_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "num_samples = 1000000\n",
    "num_attributes = 10\n",
    "\n",
    "# Generate random covariance matrix\n",
    "cov = np.random.rand(num_attributes, num_attributes)\n",
    "\n",
    "# Set half of the covariances to be less than 0.3\n",
    "cov[cov < 0.6] = np.random.uniform(0, 0.2, size=np.sum(cov < 0.6))\n",
    "\n",
    "# Set remaining covariances to be greater than 0.3\n",
    "cov[cov >= 0.6] = np.random.uniform(0.2, 0.8, size=np.sum(cov >= 0.6))\n",
    "\n",
    "# Make the covariance matrix symmetric\n",
    "cov = np.triu(cov) + np.triu(cov, 1).T\n",
    "# 计算特征值分解\n",
    "eigenvalues, eigenvectors = np.linalg.eig(cov)\n",
    "\n",
    "# 将负特征值设置为零\n",
    "eigenvalues[eigenvalues < 0] = 0\n",
    "\n",
    "# 重新构造对称半正定矩阵\n",
    "cov = np.dot(eigenvectors, np.dot(np.diag(eigenvalues), eigenvectors.T))\n",
    "np.fill_diagonal(cov, 1)\n",
    "# Generate samples from the multivariate uniform distribution\n",
    "data = np.random.laplace(loc=0, scale=1, size=(num_samples, num_attributes))\n",
    "\n",
    "# Apply the covariance matrix to the data\n",
    "data = np.dot(data, np.linalg.cholesky(cov).T)\n",
    "\n",
    "# Scale the data to the range of 0-63\n",
    "data_min = np.min(data)\n",
    "data_max = np.max(data)\n",
    "scaled_data = (data - data_min) / (data_max - data_min) * 63\n",
    "\n",
    "# Round the scaled data to the nearest integer\n",
    "rounded_data = np.round(scaled_data)\n",
    "rounded_data = rounded_data.astype(int)\n",
    "print(np.max(rounded_data))\n",
    "print(np.min(rounded_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "x = np.arange(64)\n",
    "plt.hist(rounded_data, bins = len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = ['attribute' + str(i) for i in range(10)]\n",
    "df1 = pd.DataFrame(rounded_data, columns=attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存为CSV文件\n",
    "df2.to_csv('uniform_epsilon.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 合并两个数据框\n",
    "merged_df = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "# # 按指定列和Grouper进行划分\n",
    "# grouped_df = merged_df.groupby('laplace1')\n",
    "\n",
    "# # 保存划分后的结果为CSV文件\n",
    "# grouped_df = grouped_df.apply(lambda merged_df: merged_df.sort_values(by=['laplace1']).reset_index(drop=True))\n",
    "merged_df.to_csv('1000000_laplace_64_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of samples and the number of attributes\n",
    "num_samples = 1000000\n",
    "num_attributes = 10\n",
    "\n",
    "# Generate random covariance matrix\n",
    "covariance_matrix = np.random.rand(num_attributes, num_attributes)\n",
    "\n",
    "# Set half of the covariances to be less than 0.3\n",
    "covariance_matrix[covariance_matrix < 0.6] = np.random.uniform(0, 0.2, size=np.sum(covariance_matrix < 0.6))\n",
    "\n",
    "# Set remaining covariances to be greater than 0.3\n",
    "covariance_matrix[covariance_matrix >= 0.6] = np.random.uniform(0.2, 0.8, size=np.sum(covariance_matrix >= 0.6))\n",
    "\n",
    "# Make the covariance matrix symmetric\n",
    "covariance_matrix = np.triu(covariance_matrix) + np.triu(covariance_matrix, 1).T\n",
    "# Fill the diagonal with 1's\n",
    "# 计算特征值分解\n",
    "eigenvalues, eigenvectors = np.linalg.eig(covariance_matrix)\n",
    "\n",
    "# 将负特征值设置为零\n",
    "eigenvalues[eigenvalues < 0] = 0\n",
    "\n",
    "# 重新构造对称半正定矩阵\n",
    "covariance_matrix = np.dot(eigenvectors, np.dot(np.diag(eigenvalues), eigenvectors.T))\n",
    "np.fill_diagonal(covariance_matrix, 1)\n",
    "print(covariance_matrix)\n",
    "# Generate the dataset\n",
    "data = np.random.multivariate_normal(mean=[0] * num_attributes, cov=covariance_matrix, size=num_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Set the random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define the number of samples and the number of attributes\n",
    "num_samples = 10000000\n",
    "num_attributes = 10\n",
    "\n",
    "# Generate random samples from a multivariate distribution\n",
    "# data = np.random.normal(loc=0, scale=1, size=(num_samples, num_attributes))\n",
    "data = np.random.laplace(loc=0, scale=1, size=(num_samples, num_attributes))\n",
    "# data = np.random.uniform(low=0, high=1, size=(num_samples, num_attributes))\n",
    "# Scale the data to the range of 0-63\n",
    "data_min = np.min(data)\n",
    "data_max = np.max(data)\n",
    "scaled_data = (data - data_min) / (data_max - data_min) * 63\n",
    "\n",
    "# Round the scaled data to the nearest integer\n",
    "rounded_data = np.round(scaled_data)\n",
    "rounded_data = rounded_data.astype(int)\n",
    "print(np.max(rounded_data))\n",
    "print(np.min(rounded_data))\n",
    "rounded_data[np.isnan(rounded_data)] = 0\n",
    "attributes = ['attribute' + str(i) for i in range(10)]\n",
    "df1 = pd.DataFrame(rounded_data, columns=attributes)\n",
    "\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机种子，以确保结果可重复\n",
    "np.random.seed(42)\n",
    "\n",
    "# 生成100000行2列的数据\n",
    "rows = len(df1)\n",
    "cols = 4\n",
    "\n",
    "# 生成属性名\n",
    "attributes = ['uniform1', 'normal1', 'laplace1', 'prob1']\n",
    "\n",
    "# 生成数据\n",
    "uniform_data = np.random.randint(1, 11, size=(rows, 1))\n",
    "normal_data = np.random.normal(5.5, 2.5, size=(rows, 1))\n",
    "normal_data = np.clip(normal_data, 1, 10)\n",
    "laplace_data = np.random.laplace(5, 2, size=(rows, 1))\n",
    "laplace_data = np.clip(laplace_data, 1, 10)\n",
    "prob_data = np.random.choice([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], size=(rows, 1), p=[0.18, 0.18, 0.18, 0.0925, 0.0925, 0.0925, 0.0925, 0.03, 0.03, 0.03])\n",
    "\n",
    "# 将浮点型数据转换为整型\n",
    "uniform_data = uniform_data.astype(int)\n",
    "normal_data = normal_data.astype(int)\n",
    "laplace_data = laplace_data.astype(int)\n",
    "prob_data = prob_data.astype(int)\n",
    "\n",
    "# 创建DataFrame\n",
    "df2 = pd.DataFrame(np.concatenate([uniform_data, normal_data, laplace_data, prob_data], axis=1), columns=attributes)\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 合并两个数据框\n",
    "merged_df = pd.concat([df1, df2], axis=1)\n",
    "\n",
    "# # 按指定列和Grouper进行划分\n",
    "# grouped_df = merged_df.groupby('laplace1')\n",
    "\n",
    "# # 保存划分后的结果为CSV文件\n",
    "# grouped_df = grouped_df.apply(lambda merged_df: merged_df.sort_values(by=['laplace1']).reset_index(drop=True))\n",
    "merged_df.to_csv('10000000_laplace_64_.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('usa_00010.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipumsdf = pd.read_csv('1000000_ipums_64.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "x = np.arange(64)\n",
    "plt.hist(ipumsdf['attribute8'], bins = len(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
